---
title: "03-exercises"
author: "Guzel Akhmadullina"
date: "April 20, 2016"
output: html_document
---

## Readings

***APM***

- Chapter 4 "Over Fitting and Model Tuning"
- Chapter 12.2 "Logisitic Regression""


## Miscellaneous

I am still struggling with names ...

- Please send me your picture


## Assignment 

Note: The following will set-up your environment for this exercise. If you get an error stating that the packages have not been found, you need to install those packages.


```{r,echo=FALSE, warning=FALSE, message=FALSE}

packs <-  c('AppliedPredictiveModeling', 'ggplot2', 'magrittr', 'dplyr', 'caret', 'MASS')

for( nm in packs ) { 
  # message(nm)
  if( ! nm  %in% installed.packages()[,1]  ) install.packages(nm)
  library(nm, character.only = TRUE)
}

# Load data set into environment
data(FuelEconomy)     # See ?cars2010
fe <- dplyr::bind_rows(cars2010, cars2011, cars2012)    # Define Da


data("GermanCredit")  # see GermanCredit

... = NULL  # Needed for aesthetics 

```


## StepAIC


Using Fuel Economy data set from the **AppliedPredictiveModeling** Package.
- fit the simplest possible model using lm
- Use MASS::StepAIC to improve the model using forward stepwise regression
- Fit the "full" model using lm
- USe MASS::StepAIC to improve the model using backward stepwise regression 

```{r}

names(fe)  %>% cat(sep=" + ")
form = ~ EngDispl + NumCyl + Transmission + AirAspirationMethod + NumGears + TransLockup + TransCreeperGear + DriveDesc + IntakeValvePerCyl + ExhaustValvesPerCyl + CarlineClassDesc + VarValveTiming + VarValveLift

  # Your work here

fit.min <- lm( FE ~ 1, fe )
fit.min.forward <- stepAIC( fit.min, scope = form, direction = "forward")
# summary(fit.min.forward)
# coef.fit.min.forward = coef(fit.min.forward) 
 
fit.full <- lm( FE ~ ., fe )
fit.full.backward <- stepAIC( fit.full, scope = ~ 1, direction = "backward")
# summary(fit.full.backward)
# coef.fit.full.backward = coef(fit.full.backward) 
```

- Are they the same model? If not why?  Which is better? JUstify your answer.

FORWARD SELECTION:  

AIC=3662.6

FE = EngDispl + CarlineClassDesc + DriveDesc + Transmission + NumCyl + <span style="color:red">IntakeValvePerCyl</span> + VarValveLift + TransLockup + NumGears
    
BACKWARD SELECTION: 

AIC=3663.23

FE = EngDispl + NumCyl + Transmission + <span style="color:red">AirAspirationMethod</span> + NumGears + TransLockup + <span style="color:red">TransCreeperGear</span> + DriveDesc + <span style="color:red">ExhaustValvesPerCyl</span> + CarlineClassDesc + VarValveLift  

Forward and Backward selections gave different models, since they evaluate the model on each step, and decide whether to add/drop a variable to/from the model. fit.min.forward is better since AIC coefficient is smaller to forward selection. But if we look to the summary of the models, there are some insignificant variables in both of the models. For cathegorical variables, we need to introduce dummy variables; for continuos - take out from the model.



## Logsitic and Inverse Logistic Transformation 

- Write an R function for the logistic function. The function should accept a `numeric` vector with values `[-Inf,Inf]` and produce a numeric vector in the the range `[0,1]`.

- Plot the logistic function from  `[-10,10]`

- Write a R function for the inverse logistic function. The function should accept a `numeric` vector with values `[0,1]` and prodcuce a numeric vector in the range `[-Inf,Inf]`

- Plot the Inverse Logistic function from `[0,1]`


**Hint:** For plotting curves see `?graphics::curve` or `?ggplot2::stat_function`


```{r}

logistic <- function(x) { 
    return (1/(1 + exp(-x)))  
}

qplot( -10:10, logistic(-10:10) )

logistic_inv <- function(y) { 
    return( log(y/(1-y)) )  
}

y<- seq(0, 1, length.out = 100)
qplot( logistic_inv(y), y)

```

**NOTE"** These functions are quite handy, in evaluating logistic regression results. You may want to save these functions in your own package.  

```{r}
# DO NOT EDIT

c(-Inf,0,Inf) %>% logistic

c(0,0.5,1) %>% logistic_inv

```


## German Credit Model

Using the GermanCredit data from the **Caret** package/ UCI Machine Learning Library, create a model for `Class` ("Good" vs. "Bad" ). Show your model performance.  

```{r}

# summary(GermanCredit)
# variable.names(GermanCredit)
# GermanCredit[!complete.cases(GermanCredit),]
# any(is.na(GermanCredit))


fit.full.glm <- glm (Class ~ ., data = GermanCredit, family = "binomial")
fit.full.glm.backward <- stepAIC (fit.full.glm, scope = ~ 1, direction = "backward") #AIC=966.57
summary(fit.full.glm.backward)

fit.min.glm <- glm (Class ~ 1, data = GermanCredit, family = "binomial")
names(GermanCredit)  %>% cat(sep=" + ")
form = ~ Duration + Amount + InstallmentRatePercentage + ResidenceDuration + Age + NumberExistingCredits + NumberPeopleMaintenance + Telephone + ForeignWorker + Class + CheckingAccountStatus.lt.0 + CheckingAccountStatus.0.to.200 + CheckingAccountStatus.gt.200 + CheckingAccountStatus.none + CreditHistory.NoCredit.AllPaid + CreditHistory.ThisBank.AllPaid + CreditHistory.PaidDuly + CreditHistory.Delay + CreditHistory.Critical + Purpose.NewCar + Purpose.UsedCar + Purpose.Furniture.Equipment + Purpose.Radio.Television + Purpose.DomesticAppliance + Purpose.Repairs + Purpose.Education + Purpose.Vacation + Purpose.Retraining + Purpose.Business + Purpose.Other + SavingsAccountBonds.lt.100 + SavingsAccountBonds.100.to.500 + SavingsAccountBonds.500.to.1000 + SavingsAccountBonds.gt.1000 + SavingsAccountBonds.Unknown + EmploymentDuration.lt.1 + EmploymentDuration.1.to.4 + EmploymentDuration.4.to.7 + EmploymentDuration.gt.7 + EmploymentDuration.Unemployed + Personal.Male.Divorced.Seperated + Personal.Female.NotSingle + Personal.Male.Single + Personal.Male.Married.Widowed + Personal.Female.Single + OtherDebtorsGuarantors.None + OtherDebtorsGuarantors.CoApplicant + OtherDebtorsGuarantors.Guarantor + Property.RealEstate + Property.Insurance + Property.CarOther + Property.Unknown + OtherInstallmentPlans.Bank + OtherInstallmentPlans.Stores + OtherInstallmentPlans.None + Housing.Rent + Housing.Own + Housing.ForFree + Job.UnemployedUnskilled + Job.UnskilledResident + Job.SkilledEmployee + Job.Management.SelfEmp.HighlyQualified
fit.min.glm.forward <- stepAIC (fit.min.glm, scope = form, direction = "forward") #AIC=958.15


#library(corrgram)
#corrgram(GermanCredit)
```



## Iterative Correlated Feature Removal 

- Implement Kuhn's iterative feature removal function described in **APM** Section 3.5, page 47

```{r}

ICFR <- function(DF, cutoff){
  
  COR<-cor(DF)
  max.cor <- max(abs(COR[COR!=1]))
  
  while (max.cor > cutoff){
  
      cor.pair <- row.names(which(abs(COR) == max.cor, arr.ind = TRUE))
      
      var.max.cor <- cor.pair[ 
        which(rowMeans(abs(COR[cor.pair, ])) == max(rowMeans(abs(COR[cor.pair, ]))), arr.ind = TRUE) ]
            
      COR <- subset( COR, select = - get(var.max.cor))
      COR <- COR[rownames(COR)!= var.max.cor, ]
     
      DF <- DF[, names(DF)!=as.symbol(var.max.cor)] 
      max.cor <- max(abs(COR[COR!=1]))
      
      }
  
  return(DF)
}


set.seed(100) 
x1<-rnorm(100) 
x2<-rnorm(100)
x3<-rnorm(100)
x4<-rnorm(100)
x5<-rnorm(100)
x6<-rnorm(100)
DF <- data.frame(x1, x2, x3, x4, x5, x6)
ICFR(DF, 0.2)
#findCorrelation(COR, cutoff = .1)
```


## Synthetic Data (Optional)

Sometimes it is useful to "synthesize" feature data for to understand how a certain model behaves. 
Sythesize the following features 1000-element vectors: 

- x1: a normally distributed variable with `mean = 20` and standard deviation = 20 (`sd=8`).
- x2: a log-normally distributed feature with `meanlog = 1`, `sdlog=1.2`
- x3: a uniformly distributed feature with `min=0` and `max=50`. 

```{r}
nsamples = 20

x1 <- rnorm(nsamples,20,20)  
x2 <- rlnorm(nsamples, meanlog=1, sdlog = 1.2)
x3 <- runif(nsamples,0,50)

```

Next synthesis a response, `y` using the betas provided and an intercept that is normally distributed at 20 with standard deviation of 2. (**Hint:**  The betas thought of can be a vector or matrix)



```{r}

beta0 <- rnorm(nsamples,20,2)  # intercept!
beta1 <- 2.3
beta2 <- 4
beta3 <- 7

betas <- matrix( c(2.3, 4, 7), nrow=1  )  # 1x4 matrix

# x0 <- rep(1,nsamples) 

X  <- cbind(x1,x2,x3)  # 1000x4

y <- betas %*% t(X) %>% t
y <- y + beta0

qplot(y)
dat <- data.frame(y,X)

fit <- lm( y ~ . , dat )

coef(fit)

fit
```

- Did you recover the betas? 
- Is the model good?
- What happens if increase the value of `nsamples`? Decrease it?
- What transformations would you apply to x1? x2? x3? 

